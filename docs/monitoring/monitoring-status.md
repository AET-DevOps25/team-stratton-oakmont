# üìä TUM Study Planner - Monitoring Status

## üéØ Current Status: ‚úÖ PRODUCTION READY

Your monitoring stack is successfully deployed and configured for production use with Artemis best practices.

## üèóÔ∏è Architecture Overview

### Monitoring Stack
- **Namespace**: `tum-study-planner-monitoring` (dedicated, isolated)
- **Prometheus**: Metrics collection with persistent storage
- **Grafana**: Visualization dashboards 
- **AlertManager**: Slack notifications to `#alerts-warning`

### Application Metrics
- **Program Catalog Service**: ‚úÖ Exposing metrics on port 8081
- **AI Advisor Service**: ‚úÖ Exposing metrics on port 8081
- **Study Plan Service**: ‚úÖ Exposing metrics on port 8081
- **User Auth Service**: ‚úÖ Exposing metrics on port 8081

## üîç Key Features Implemented

### ‚úÖ Production-Grade Setup
- [x] **Dedicated Namespace**: All monitoring runs in `tum-study-planner-monitoring`
- [x] **Persistent Storage**: PVCs for data retention across pod restarts
- [x] **RBAC**: Proper service accounts and permissions
- [x] **Label-based Discovery**: Automatic service/pod discovery via labels
- [x] **Version Metrics**: All Spring Boot services expose build/version info

### ‚úÖ Alerting & Notifications
- [x] **Slack Integration**: Single channel (`#alerts-warning`) for all alerts
- [x] **Secure Configuration**: Webhook stored as Kubernetes secret
- [x] **Alert Rules**: Critical and warning alerts for common issues
- [x] **Automated Setup**: Script for easy Slack configuration

### ‚úÖ Monitoring Coverage
- [x] **Service Health**: Uptime monitoring for all microservices
- [x] **Performance**: Response time and error rate tracking
- [x] **Infrastructure**: Kubernetes node and pod health
- [x] **Version Tracking**: Build and deployment information

## üìä Deployed Resources

### Monitoring Namespace
```bash
kubectl get all -n tum-study-planner-monitoring
```
- **Prometheus**: Metrics collection server
- **Grafana**: Dashboard and visualization
- **AlertManager**: Alert routing and notifications
- **PersistentVolumes**: Data storage for all components

### Application Services (with Monitoring)
```bash
kubectl get services -n tum-study-planner -l monitoring=enabled
```
- All microservices configured with monitoring labels
- Management ports exposed for Prometheus scraping
- Version/build metrics available at `/actuator/prometheus`

## üö® Alert Configuration

### Alert Types (All ‚Üí #alerts-warning)
| Alert | Trigger | Severity | Description |
|-------|---------|----------|-------------|
| ServiceDown | Service unavailable >1min | üö® Critical | Service completely down |
| HighResponseTime | >500ms response >2min | ‚ö†Ô∏è Warning | Performance degradation |
| HighErrorRate | >10% errors >2min | ‚ö†Ô∏è Warning | High error rate |
| PodRestartingTooMuch | Frequent restarts | ‚ö†Ô∏è Warning | Pod instability |
| KubernetesNodeNotReady | Node issues >5min | üö® Critical | Infrastructure problems |
| KubernetesPodCrashLooping | Crash loops | ‚ö†Ô∏è Warning | Application crashes |

### Slack Channel Setup
- **Channel**: `#alerts-warning`
- **Critical Alerts**: Red color with üö® prefix
- **Warning Alerts**: Orange color with ‚ö†Ô∏è prefix
- **Resolved Notifications**: Automatic resolution messages

## üõ†Ô∏è Management Commands

### Check Status
```bash
# Monitoring pods
kubectl get pods -n tum-study-planner-monitoring

# Application services with monitoring
kubectl get services -n tum-study-planner -l monitoring=enabled

# Secrets and configuration
kubectl get secrets -n tum-study-planner-monitoring
```

### Access UIs
```bash
# Prometheus
kubectl port-forward -n tum-study-planner-monitoring svc/prometheus 9090:9090
# Visit: http://localhost:9090

# Grafana  
kubectl port-forward -n tum-study-planner-monitoring svc/grafana 3000:3000
# Visit: http://localhost:3000

# AlertManager
kubectl port-forward -n tum-study-planner-monitoring svc/alertmanager 9093:9093
# Visit: http://localhost:9093
```

### Update Slack Configuration
```bash
# Use the automated script
./scripts/setup-slack-alerts.sh

# Or manually update secret
kubectl create secret generic alertmanager-secrets \
  --namespace=tum-study-planner-monitoring \
  --from-literal=slack-webhook-url="YOUR_NEW_WEBHOOK_URL" \
  --dry-run=client -o yaml | kubectl apply -f -
```

## üß™ Testing Your Setup

### 1. Verify Metrics Collection
```bash
# Check if services are being scraped
kubectl port-forward -n tum-study-planner-monitoring svc/prometheus 9090:9090
# Visit: http://localhost:9090/targets
```

### 2. Test Alert Delivery
```bash
# Temporarily stop a service (triggers ServiceDown alert)
kubectl scale deployment program-catalog-service --replicas=0 -n tum-study-planner

# Wait 2-3 minutes, then check Slack for alert

# Restore service
kubectl scale deployment program-catalog-service --replicas=1 -n tum-study-planner
```

### 3. Check Version Metrics
Visit Prometheus and query: `application_info`
You should see version/build information for all services.

## üìö Next Steps (Optional)

### Enhanced Dashboards
- Import additional Grafana dashboards for Spring Boot applications
- Create custom dashboards for business metrics
- Set up dashboard alerting in Grafana

### Advanced Alerting
- Configure different alert severities for different times of day
- Add more granular alerts for specific application metrics
- Integrate with incident management tools (PagerDuty, Opsgenie)

### Security Enhancements
- Implement network policies for monitoring namespace
- Add authentication for Grafana (LDAP/OAuth)
- Enable TLS for all monitoring communications

## üéâ Congratulations!

Your TUM Study Planner monitoring setup follows Artemis production best practices:
- ‚úÖ Isolated monitoring namespace
- ‚úÖ Persistent data storage
- ‚úÖ Automatic service discovery
- ‚úÖ Version/build tracking
- ‚úÖ Production-ready alerting
- ‚úÖ Secure configuration management

Your monitoring stack is ready for production use! üöÄ
